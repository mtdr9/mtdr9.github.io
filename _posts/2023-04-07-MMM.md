---
layout: post
title: Media Mix Modeling - What, Why, and How?
subtitle: And a comparison vs. attribution modeling
image: /img/MMM/water_flows.png
tags: [machine learning, attribution, optimization]
---

How should I allocate spend to different marketing channels? If you're managing a company's marketing budget, this may be the most essential question to maximize your revenue/profits given limited spend. There's essentially two ways to answer it scientifically:
1. Look from the top down: take all of your data on which days (/weeks, etc.) you spent on which channels, and when your revenue came in. Then, use a **media mix model** (MMM - also called marketing mix modeling) to determine which channels drove your KPIs, and which contributed less or none.
2. Look from the bottom up: use as granular data as possible on which customers saw which ads, and if/when those customers converted. Use a **multi-touch attribution model** (MTA) to determine which ads drove each purchase/conversion, then sum your results by channel to see how valuable each one is.

At Wayfair, we focused on MTA to get hyper granular results, which let us optimize our marketing channels down to the performance of an individual ad. This worked very well as a mature company (sorry Wayfair you're not a startup anymore) with mountains of data to analyze and a large staff of data scientists to build and validate a complex model. It also let us analyze the customer journey at a deep level, looking at what touchpoints contributed to success, for example. MMM, on the other hand, provides a high-level view only, by taking in aggregate data by channel. However, even at a company with MTA, running MMM a couple times a year can help validate what the attribution is showing, providing a general view of which channels are driving outcomes. And at a smaller firm, it can be the fastest way to allocate spend across the department.

## How to run Media Mix Modeling (MMM)

I'll be using an example dataset called 'df', a Python Pandas dataframe, which contains a couple hundred weeks of spend data and the conversions they generated. The 3 channels are 'TV', 'radio', and 'newspaper', and conversions are named 'sales'. For this example, the ETL and data cleaning steps are already complete. The full code is available [here](https://github.com/mtdr9/public_projects/blob/289ef4b65582fb51b296075c55bd0aeeab4b9b68/Marketing%20Mix%20Model.ipynb) if you'd like to see the whole process.

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>TV</th>
      <th>radio</th>
      <th>newspaper</th>
      <th>sales</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>230.1</td>
      <td>37.8</td>
      <td>69.2</td>
      <td>22.1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>44.5</td>
      <td>39.3</td>
      <td>45.1</td>
      <td>10.4</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17.2</td>
      <td>45.9</td>
      <td>69.3</td>
      <td>9.3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>151.5</td>
      <td>41.3</td>
      <td>58.5</td>
      <td>18.5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>180.8</td>
      <td>10.8</td>
      <td>58.4</td>
      <td>12.9</td>
    </tr>
  </tbody>
</table>
</div>

### EDA

Step one when working with a new dataset is always to understand what you're working with. I'll create a correlation matrix and pair plot to see how these features are related to each other.

```python
corr = df.corr()
sns.heatmap(corr, xticklabels = corr.columns, yticklabels = corr.columns, annot = True, cmap = sns.light_palette(color='seagreen', as_cmap=True))
```

![png](http://mattdorros.com/img/MMM/corr_matrix.png)

It looks like TV is most heavily correlated with sales, and could be our best bet for driving performance. Also, radio and newspaper tended to be used together more often than any other pair of channels, even though so far newspaper isn't very correlated with sales. Perhaps we'll be able to trim our newspaper usage.

Next let's make a pair plot to show a) scatterplots of the relationship between different variables, and b) histograms along the diagonal that show the frequency of each value.

```python
# How simple is this? Python is amazing
sns.pairplot(df)
```

![png](http://mattdorros.com/img/MMM/pairplot.png)

There's a lot to take away from this. Starting on the right with channels' relationship with sales, it looks like TV has a clear relationship with sales, especially when considering no TV vs. some TV, while radio may have a looser relationship, and newspaper looks totally irrelevant given its random distribution. TV looks like it could have an exponential relationship, at least at first. Next, it looks like newspaper's data is skewed to the left, with most weeks spending <$50 but some weeks spending close to $100; we may want to correct that using a transformation like [Box Cox](https://scientistcafe.com/ids/resolve-skewness.html). Lastly, there's no obvious trend between the 3 independent variables.

### Model selection, training, and testing

Let's keep this really simple. The model building process looks like this:
1. Identify models that fit the problem and the data at hand
2. Build said models, and measure how accurately they can predict your test data
3. Choose the model that performed the best and productionize it

We're working with a regression problem with a continuous dependent variable and 3 independent variables, so our two main choices are a linear model or a tree-based model. Let's dive into the most common algorithms we can consider:
* "Linear" models (called so since they're variants of linear regression, but some of them can handle non-linear data)
    * Multiple linear regression - this model fits the scenario I described above, but our data isn't linear, so it's not likely to perform very well. However, if we adjust our data by e.g. taking the log of the dependent variable, we may be able to fit a line to it; this would be called a log-linear model and is likely worth trying since it's so easily interpretable if it works.
    * Ridge & lasso regression - these models correct for collinear data. We saw in our EDA that most of the variables weren't very highly correlated, so they're not likely to help too much.
    * Polynomial regression - now we're talking. This is linear regression but as applied to non-linear data. Sounds promising!
* Tree-based models
    * Decision tree - sure we could use one decision tree, but whoever uses one when the whole forest is available? And it's still environmentally friendly
    * Random forest - these models are highly accurate and are not particularly prone to overfitting. Since our data complexity is low, the only major disadvantage is that they're not very interpretable
    * Gradient boosting and XGBoost - these models can be even more accurate than random forests, although they're sensitive to outliers and can overfit. Our data doesn't have too many outliers though, except arguably on days where spend was close to 0, so this looks promising

If I were getting paid to solve this problem, I'd test out log-linear regression, polynomial regression, random forests, gradient boosting, and XGBoost. But I'm not, so I tried log-linear regression and random forests, so that I'd get results from both the "linear" and tree-based models. Spoiler alert: random forests won, so let's focus on that. (Again, the full analysis is available [here](https://github.com/mtdr9/public_projects/blob/main/Marketing%20Mix%20Model.ipynb)).

```python
# create our X and y variables from our df. y is simply the sales data
X = df.iloc[:,0:3]
y = df.iloc[:,3]

# create a randomized split of training and testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=7)
# create the random forest model and train it
rf_model = RandomForestRegressor(random_state=7)
rf_model.fit(X_train, y_train)
# create predictions using our training data, then save them
pred = rf_model.predict(X_test) 
rf_results = pd.DataFrame({'actual': y_test, 'pred':pred})

# evaluate our results with a plot of predictions vs. actuals. Sort by actual sales for readability
rf_results.sort_values(inplace=True,by='actual')
rf_results.reset_index(inplace=True, drop=True)
plt.plot(rf_results.index.values, rf_results.actual.values, color='b', label='actual')
plt.plot(rf_results.index.values, rf_results.pred.values, color='r', label='predicted')
plt.xlabel('Observation #')
plt.ylabel('Sales')
plt.legend(loc='upper left')
plt.show()
```

![png](http://mattdorros.com/img/MMM/rf_preds.png)

The plot above shows our random forest's predictions for sales vs. the test data (which we created by splitting our data set into 25% test data and 75% training data). It looks pretty reasonable! Let's look at some aggregate measures of error.

```python
# random forest model error
rf_actual = rf_results['actual'].values
rf_pred = rf_results['pred'].values

print(metrics.mean_absolute_error(rf_actual, rf_pred))
print(metrics.mean_squared_error(rf_actual, rf_pred))
# taking the square root gets us root mean squared error
print(np.sqrt(metrics.mean_squared_error(rf_actual, rf_pred)))
```

MAE = 0.73  
MSE = 1.03  
RMSE = 1.02  

This was a significant improvement over e.g. the log linear model error, which had a mean average error of 1.23, and a mean squared error of 2.95. This means that while the random forest had about half to two-thirds as much error on average (0.73 vs. 1.23) it had just one third as much error when using a metric that punishes outliers (1.03 vs. 2.95). [(Refresher on types of error here)](https://medium.com/analytics-vidhya/mae-mse-rmse-coefficient-of-determination-adjusted-r-squared-which-metric-is-better-cd0326a5697e).

### Budget optimization

Now that we have a working model, what does it tell us about how should we spend our marketing budget? The ideal process for this would use a multi-armed bandit test to run through the following process:
1. Feed X values into the model
2. Record the output y, and repeat until you've maximized y

Given the dataset, there are some open questions.
1. How much is a sale/conversion worth? We know marketing spend, but not marketing revenue or profit
2. What are the goals of the business right now? Maximize growth by focusing on revenue or customer acquisition? Generate profits to appease stakeholders or pay off debt obligations?
3. Are there budget or efficiency constraints, or are we free to spend as much as needed to hit the business goal?
Without these pieces of business context, we can't truly optimize the budget. What we can do is figure out optimal budget allocations at different levels of spend. What levels of spend should we focus on? 

Looking at our training data, we have weekly spend figures between about $0 and $400. Random forests perform worse at extrapolating than interpolating, so let's limit our results to this range. We'll seek to maximize sales within that budget.

```python
# Plot total spend vs. sales in our training data
# Note that there's also a clear relationship between spend and sales
plt.scatter(X.sum(axis=1),y)
plt.xlabel('Total spend $')
plt.ylabel('Sales $')
```

![png](http://mattdorros.com/img/MMM/total_spend.png)

As mentioned, a multi-armed bandit test would be the ideal way to find the optimal allocation. But given that it's a 3-variable problem, we can actually calculate a solution by trying all possible values (in multiples of 5).

```python
# create 500K options to test
import itertools
tv = list(range(0-ns, 401-ns, 5))
radio = list(range(0-ns, 401-ns, 5))
np = list(range(0-ns, 401-ns, 5))
s = [tv, radio, np]

# run each budget allocation through our random forest\
y_three = rf_model.predict(trials_three)

# create dataframe and pull the maximum sales we can generate; and of those, the minimum spend
trials_three = pd.DataFrame(list(itertools.product(*s)), columns=['TV','radio','newspaper'])
trials_three['total_spend'] = trials_three.sum(axis=1)
trials_three['sales'] = y_three
max_alloc = trials_three.query('sales == sales.max()')
max_alloc.query('total_spend == total_spend.min()')
```

### The Verdict

There we have it: in experiment 361,670 we found our ideal allocation. 275 to TV, 50 to radio, and 25 to newspaper, assuming we want to maximize sales but constrain our budget to be <$400. I'd recommend shifting spend from radio and newspaper to TV to accomplish this, ideally in an A/B testing framework, such as by running a geographical study using the new approach as the treatment.

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>TV</th>
      <th>radio</th>
      <th>newspaper</th>
      <th>total_spend</th>
      <th>sales</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>361670</th>
      <td>275</td>
      <td>50</td>
      <td>25</td>
      <td>350</td>
      <td>26.386</td>
    </tr>
  </tbody>
</table>
</div>

### Next steps for model development
With limited time, there were plenty of optimizations I skipped over that could be used to refine our answer.
* In my opinion the largest gap is that there's no accounting for effect over time: each week is treated as separate. In reality, TV advertising one week likely increases conversions for the next few weeks as well. To account for this, we'd want to use Bayesian methods to account for carryover and shape effects, as described [here](https://towardsdatascience.com/carryover-and-shape-effects-in-media-mix-modeling-paper-review-fd699b509e2d).
* Similarly, there's no accounting for seasonality, and spend in certain areas may work better some weeks/months than others. Getting time series data could let us add that as a feature to analyze.
* We didn't optimize the RF's hyperparameters; doing so can lead to increased predictive accuracy.
* We could have used sampling to find the confidence and prediction intervals for the model. This would tell us how confident each estimate is. [Here](https://stats.stackexchange.com/questions/562058/random-forests-confidence-intervals-and-prediction) is one methodology to do so.
* Feature selection: while testing model error, or once we've selected a model, it's often helpful to prune features that aren't contributing to its predictive power. However, given that we only have 3 variables, and each has an effect on sales, I elected to keep all three in the model.

## Overall takeaways
In order to make a recommendation, we had to make some assumptions about the business' needs and the value of conversions/sales. In a real business setting, understanding the goals of the business and the value of our conversions would be some of the first steps we'd need to take while planning out our analysis. But now that we have a recommendation - move about 80% of spend to TV, while keeping both radio and newspaper but as smaller channels - we'd want to run A/B testing to validate these results. Since there's still improvements to be made in the model, we'd want to keep this test small and cheap, to validate investing more time into our predictions; or perhaps, depending on the exact cost of that A/B test, run it after building seasonality and the impact of spend over time into our model. 

In sum, this media mix model was a fairly quick way to identify that the business is likely not investing enough in TV. And really, it should consider adding a 4th, 5th, or 10th marketing channel! 
